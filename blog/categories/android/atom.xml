<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Android | 胡凯]]></title>
  <link href="http://hukai.me/blog/categories/android/atom.xml" rel="self"/>
  <link href="http://hukai.me/"/>
  <updated>2014-06-24T10:09:25+08:00</updated>
  <id>http://hukai.me/</id>
  <author>
    <name><![CDATA[Kesen Hoo]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Android Training - Volley(Lesson 1 - 发送简单的请求)]]></title>
    <link href="http://hukai.me/blog/android-training-volley-simple-request/"/>
    <updated>2014-06-24T09:00:00+08:00</updated>
    <id>http://hukai.me/blog/android-training-volley-simple-request</id>
    <content type="html"><![CDATA[<p>使用Volley的方式是，你通过创建一个<code>RequestQueue</code>并传递<code>Request</code>对象给它。RequestQueue管理工作线程用来执行网络操作，从Cache中读取与写入数据，以及解析Http的响应内容。<code>Requests</code>执行raw responses的解析，Volley会把响应的数据分发给主线程。</p>

<p>这节课会介绍如何使用<code>Volley.newRequestQueue</code>这个建立请求队列的方法来发送一个请求，在下一节课<a href="request-queue.html">建立一个请求队列Setting Up a RequestQueue</a>中会介绍你自己如何建立一个请求队列。</p>

<p>这节课也会介绍如何添加一个请求到RequesutQueue以及如何取消一个请求。</p>

<h2>Add the INTERNET Permission</h2>

<p>为了使用Volley，你必须添加<code>android.permission.INTERNET</code>权限到你的manifest文件中。没有这个权限，你的app将无法访问网络。</p>

<h2>Use newRequestQueue</h2>

<p>Volley提供了一个简便的方法：<code>Volley.newRequestQueue</code>用来为你建立一个<code>RequestQueue</code>，使用默认值，并启动这个队列。例如：</p>

<!-- More -->


<p>```java
final TextView mTextView = (TextView) findViewById(R.id.text);
...</p>

<p>// Instantiate the RequestQueue.
RequestQueue queue = Volley.newRequestQueue(this);
String url ="http://www.google.com";</p>

<p>// Request a string response from the provided URL.
StringRequest stringRequest = new StringRequest(Request.Method.GET, url,</p>

<pre><code>        new Response.Listener() {
@Override
public void onResponse(String response) {
    // Display the first 500 characters of the response string.
    mTextView.setText("Response is: "+ response.substring(0,500));
}
</code></pre>

<p>}, new Response.ErrorListener() {</p>

<pre><code>@Override
public void onErrorResponse(VolleyError error) {
    mTextView.setText("That didn't work!");
}
</code></pre>

<p>});
// Add the request to the RequestQueue.
queue.add(stringRequest);
```</p>

<p>Volley总是把解析过后的数据返回到主线程中。在主线程中更加合适使用接收到到的数据用来操作UI控件，这样你可以在响应的handler中轻松的修改UI，但是对于库提供的一些其他方法是有些特殊的，例如与取消有关的。</p>

<p>关于如何创建你自己的请求队列，不要使用Volley.newRequestQueue方法，请查看<a href="request-queue.html">建立一个请求队列Setting Up a RequestQueue</a>。</p>

<h2>Send a Request</h2>

<p>为了发送一个请求，你只需要构造一个请求并通过<code>add()</code>方法添加到<code>RequestQueue</code>中。一旦你添加了这个请求，它会通过队列，得到处理，然后得到原始的响应数据并返回。</p>

<p>当你执行<code>add()</code>方法时，Volley触发执行一个缓存处理线程以及网络一系列的网络处理线程。当你添加一个请求到队列中，它将被缓存线程所捕获并触发：如果这个请求可以被缓存处理，那么会在缓存线程中执行响应数据的解析并返回到主线程。如果请求不能被缓存所处理，它会被放到网络队列中。网络线程池中的第一个可用的网络线程会从队列中获取到这个请求并执行HTTP操作，解析响应数据，把数据写到缓存中之后再把解析之后的数据返回到主线程。</p>

<p>请注意那些比较耗时的操作，例如I/O与解析parsing/decoding都是执行在工作线程。<strong>你可以在任何线程中添加一个请求，但是响应结果都是返回到主线程的。</strong></p>

<p>下图1，演示了一个请求的生命周期：</p>

<p><img src="/images/articles/volley-request.png" alt="volley-request" /></p>

<h2>Cancel a Request</h2>

<p>为了取消一个请求，对你的请求对象执行<code>cancel()</code>方法。一旦取消，Volley会确保你的响应Handler不会被执行。这意味着在实际操作中你可以在activity的<code>onStop()</code>方法中取消所有pending在队列中的请求。你不需要通过检测<code>getActivity() == null</code>来丢弃你的响应handler，其他类似<code>onSaveInstanceState()</code>等保护性的方法里面也都不需要检测。</p>

<p>为了利用这种优势，你应该跟踪所有已经发送的请求，以便在需要的时候，可以取消他们。<strong>有一个简便的方法</strong>：你可以为每一个请求对象都绑定一个tag对象。你可以使用这个tag来提供取消的范围。例如，你可以为你的所有请求都绑定到执行的Activity上，然后你可以在<code>onStop()</code>方法执行<code>requestQueue.cancelAll(this)</code> 。同样的，你可以为ViewPager中的所有请求缩略图Request对象分别打上对应Tab的tag。并在滑动时取消这些请求，用来确保新生成的tab不会被前面tab的请求任务所卡到。</p>

<p>下面一个使用String来打Tag的例子：</p>

<ul>
<li>定义你的tag并添加到你的请求任务中。</li>
</ul>


<p>```java
public static final String TAG = "MyTag";
StringRequest stringRequest; // Assume this exists.
RequestQueue mRequestQueue;  // Assume this exists.</p>

<p>// Set the tag on the request.
stringRequest.setTag(TAG);</p>

<p>// Add the request to the RequestQueue.
mRequestQueue.add(stringRequest);
```</p>

<ul>
<li>在activity的onStop()方法里面，取消所有的包含这个tag的请求任务。</li>
</ul>


<p>```java
@Override
protected void onStop () {</p>

<pre><code>super.onStop();
if (mRequestQueue != null) {
    mRequestQueue.cancelAll(TAG);
}
</code></pre>

<p>}
```</p>

<p>当取消请求时请注意：如果你依赖你的响应handler来标记状态或者触发另外一个进程，你需要为此给出有力的解释。再说一次，response handler是不会被执行的。</p>

<hr />

<p><strong>学习自<a href="http://developer.android.com/training/volley/simple.html">http://developer.android.com/training/volley/simple.html</a>,欢迎交流讨论</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Android Training - Volley(Lesson 0 - 序言)]]></title>
    <link href="http://hukai.me/blog/android-training-volley-index/"/>
    <updated>2014-06-24T08:30:00+08:00</updated>
    <id>http://hukai.me/blog/android-training-volley-index</id>
    <content type="html"><![CDATA[<p><code>Volley</code> 是一个HTTP库，它能够帮助Android apps更方便的执行网络操作，最重要的是，它更快速高效。可以通过开源的 <a href="https://android.googlesource.com/platform/frameworks/volley">AOSP</a> 仓库获取到Volley 。</p>

<hr />

<p><strong>DEPENDENCIES AND PREREQUISITES</strong></p>

<p>Android 1.6 (API Level 4) or higher</p>

<p><strong>YOU SHOULD ALSO SEE</strong></p>

<p>使用Volley来编写一个app，请参考<a href="https://github.com/google/iosched">2013 Google I/O schedule app</a>. 另外需要特别关注下面2个部分：
* <a href="https://github.com/google/iosched/blob/master/android/src/main/java/com/google/android/apps/iosched/util/ImageLoader.java">ImageLoader</a>
* <a href="https://github.com/google/iosched/blob/master/android/src/main/java/com/google/android/apps/iosched/util/BitmapCache.java">BitmapCache</a></p>

<p><strong> <a href="https://developers.google.com/events/io/sessions/325304728">VIDEO - Volley:Easy,Fast Networking for Android</a> </strong></p>

<hr />

<p>Volley 有如下的优点：</p>

<!-- More -->


<ul>
<li>自动执行网络请求。</li>
<li>高并发网络连接。</li>
<li>通过标准的HTTP的<a href="http://en.wikipedia.org/wiki/Cache_coherence%22">cache coherence</a>(高速缓存一致性)使得磁盘与内存缓存不可见(Transparent)。</li>
<li>支持指定请求的优先级。</li>
<li>支持取消已经发出的请求。你可以取消单个请求，或者指定取消请求队列中的一个区域。</li>
<li>框架容易被定制，例如，定制重试或者回退功能。</li>
<li>强大的指令(Strong ordering)可以使得异步加载网络数据并显示到UI的操作更加简单。</li>
<li>包含了Debugging与tracing工具。</li>
</ul>


<p>Volley擅长执行用来显示UI的RPC操作， 例如获取搜索结果的数据。它轻松的整合了任何协议，并输出操作结果的数据，可以是raw strings，也可以是images，或者是JSON。通过提供内置你可能使用到得功能，Volley可以使得你免去重复编写样板代码，使你可以把关注点放在你的app的功能逻辑上。</p>

<p>Volley不适合用来下载大的数据文件。因为Volley会在解析的过程中保留持有所有的响应数据在内存中。对于下载大量的数据操作，请考虑使用<a href="http://developer.android.com/reference/android/app/DownloadManager.html">DownloadManager</a>。</p>

<p>Volley框架的核心代码是托管在AOSP仓库的<code>frameworks/volley</code>中，相关的工具放在<code>toolbox</code>下。把Volley添加到你的项目中的最简便的方法是Clone仓库然后把它设置为一个library project：</p>

<ul>
<li>通过下面的命令来Clone仓库：</li>
</ul>


<p><code>git clone https://android.googlesource.com/platform/frameworks/volley</code></p>

<ul>
<li>以一个Android library project的方式导入下载的源代码到你的项目中。(如果你是使用Eclipse，请参考<a href="http://developer.android.com/tools/projects/projects-eclipse.html">Managing Projects from Eclipse with ADT</a>)，或者编译成一个<code>.jar</code>文件。</li>
</ul>


<h2>Lessons</h2>

<ul>
<li><a href="simple.html">发送一个简单的网络请求(Sending a Simple Request)</a></li>
</ul>


<p>学习如何通过Volley默认的行为发送一个简单的请求，以及如何取消一个请求。</p>

<ul>
<li><a href="request-queue.html">建立一个请求队列(Setting Up a RequestQueue)</a></li>
</ul>


<p>学习如何建立一个请求队列，以及如何实现一个单例模式来创建一个请求队列。</p>

<ul>
<li><a href="request.html">生成一个标准的请求(Making a Standard Request)</a></li>
</ul>


<p>学习如何使用Volley的out-of-the-box的请求类型(raw strings, images, and JSON)来发送一个请求。</p>

<ul>
<li><a href="request-custom.html">实现自定义的请求(Implementing a Custom Request)</a></li>
</ul>


<p>学习如何实现一个自定义的请求</p>

<hr />

<p><strong>学习自<a href="http://developer.android.com/training/volley/index.html">http://developer.android.com/training/volley/index.html</a>,欢迎交流讨论</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Android Deeper(01) - Graphic Architecture]]></title>
    <link href="http://hukai.me/blog/android-deeper-01-graphics-architecture/"/>
    <updated>2014-05-15T14:25:00+08:00</updated>
    <id>http://hukai.me/blog/android-deeper-01-graphics-architecture</id>
    <content type="html"><![CDATA[<p>Android中有几个很重要的概念Surface, SurfaceHolder, EGLSurface, SurfaceView, GLSurfaceView, SurfaceTexture, TextureView与SurfaceFlinger。</p>

<p>这篇文章会介绍Android图形架构的基本构成以及它们是如何在程序framework与多媒体系统中运作的。核心关注点是，图形数据的buffer是如何在系统中传递的。如果你曾经好奇过SurfaceView与TextureView的工作方式，或者是想知道Surface与EGLSurface是如何交互的，那么这篇文章将给你解答这些疑问。<strong>You've come to the right place.</strong></p>

<p>大多数时候，我们都不需要了解这些类的使用原理，但是学习这些它们的工作原理可以为我们提供一种Sense，用来更加高效的工作。所以这里我们学习它们是如何工作的，而不仅仅是它们是如何使用的。</p>

<h2>BufferQueue and gralloc</h2>

<p>BufferQueue and grallocGraphiccal的核心是BufferQueue,它的角色是连接产生图形数据的生产者与显示数据的消费者。生产者与消费者可以再两个不同的进程，那么数据在系统中进行传递就依赖于BufferQueue。</p>

<p>基础的用法是很直接简单的。生产者获取到一个buffer，制定了宽，高，pixel format与usage flags之后，再放到queue中。消费者获取到buffer，消费数据后，把buffer返回到queue中。</p>

<ul>
<li>生产者：dequeueBuffer() -> queueBuffer()</li>
<li>消费者：acquireBuffer() -> releaseBuffer()</li>
</ul>


<!-- More -->


<p>目前很多Android设备已经开始支持“Sync framework”。这使得系统可以结合hardware组件对graphic data做异步的操作。例如，生产者再rendering完成之前就开始提交一系列的OpenGL ES的绘制命令并参与到输出排队buffer中。Buffer队列中设置有多道栅栏，用来同步生产者与消费者的工作。这样的一个方法可以减少buffer数据在系统中传递的延迟并提升吞吐量。</p>

<p>队列的一些特性，例如最大支持Hold住的数据量，取决于生产者与消费者的共同作用。</p>

<p>BufferQueue的责任是分配Buffer,而Buffer则用来Hold Data的部分。</p>

<h3>gralloc HAL</h3>

<p>真正实行buffer allocation的操作是通过gralloc来实现的，里面的alloc()方法会根据buffer的参数：宽高，pixel format与usage flags等，进行操作。</p>

<p>gralloc不仅仅是native heap上分配内存的另外一个方式。通常alloc取决于usage flags,例如</p>

<ul>
<li> how often the memory will be accessed from software (CPU)</li>
<li> how often the memory will be accessed from hardware (GPU)</li>
<li> whether the memory will be used as an OpenGL ES ("GLES") texture(质地)</li>
<li> whether the memory will be used by a video encoder</li>
</ul>


<p>例如，如果你指定format RGBA8888，并且这个buffer可以从software层进行访问，那么alloc会使用R-G-B-A的顺序创建每一个pixel.如果你指定buffer只会被hardware访问并且会作为GLES texture的组成部分，那么alloc可以使用BGRA的顺序，这样效率更高。</p>

<h2>SurfaceFlinger and Hardware Composer</h2>

<p>如何把那些数据绘制到屏幕上，需要使用到SurfaceFlinger与Hardware Composer HAL。</p>

<p>SurfaceFlinger的作用是接收来自各处的数据，进行混合编制之后，然后进行显示。</p>

<p>当app作为foreground时，WindowManager Service会请求SurfaceFlinger绘制一个surface。SurfaceFlinger会创建一个”layer” - 它的主要组成部分是BufferQueue。此时SurfaceFlinger扮演了消费者的角色。生产者的数据是通过WindowManager传递到app，WindowManager可以直接发送frames给SurfaceFlinger。其实可以把SurfaceFlinger理解为LayerFlinger。</p>

<p>对于大多数apps来说，在任何时刻，屏幕上会有三个layer。“Status Bar”是在屏幕的最上层，“navigation bar”在屏幕的底部或者侧边，另外一层是app UI layer。一些app可能拥有更多或者更少的layer。例如默认的home app，对于wallpaper有单独设置一层layer，全屏游戏的应用会隐藏status bar的layer。Each layer都可以单独进行更新。Status与Navigation Bar是由系统process进行绘制的。app layer是由app进行绘制的。它们之间是相互独立的。</p>

<p>设备显示屏以固定的频率刷新屏幕，对于手机与平板来说，通常是60fps。刷新的频率有时是不固定的。</p>

<p>当SurfaceFlinger接收到VSYNC刷新信号时，会遍历所有的layers，寻找可以使用的新的buffers。如果找到新的buffer，那么就获取它，否则继续使用前面获取到得buffer。SurfaceFlinger总是需要有内容可以显示，所以它会持续hold住buffer。如果没有新的buffer提交到layer上，这个layer就会被SurfaceFlinger所忽略。</p>

<p>一旦SurfaceFlinger收集到了可见layer的所有buffers，它会请求Hardware composer如何进行composition的操作。</p>

<h3>Hardware Composer</h3>

<p>Hardware Composer HAL(HWC)是从3.0开始才被引入的。它首要的目的是结合可用的硬件，判断选择最有效的方式用来composite buffers。这部分的功能通常是硬件OEM厂商实现的。</p>

<p>举例说明2种compossite的方式：</p>

<ul>
<li>先画好App的UI,然后把StatusBar与BottomBar的部分绘制到UI之上，最后把这些组合好的buffer送给硬件进行显示。</li>
<li>分别传递三部分的Buffer，告诉硬件从不同的屏幕区域读取不同的buffer。
显然第2种方式会更有效率。</li>
</ul>


<p>因为不同的设备处理能力不一样，每一个layer是否需要旋转，是否有限制的显示位置等等问题。所以HWC的工作方式是这样的：</p>

<ul>
<li>SurfaceFlinger提供给HWC一个完整的layer list,并且询问系统:该如何处理这些layers?</li>
<li>HWC通过把每一个layer标记为”overlay”或者“GLES Composition”来作为应答。</li>
<li>SurfaceFlinger对那些标记为GLES Composition的layer，传递buffer给HWC，并且让HWC来处理。</li>
</ul>


<p>Overlay的方式比GLES composition的方式效率要低很多，特别是Overlay的内容为transparent时。</p>

<h3>The Need for Triple-Buffering</h3>

<ul>
<li><strong>Double-buffering</strong></li>
</ul>


<p>为了避免在显示上出现断层，系统需要是double-buffered: front buffer显示的同时，back buffer正在准备。
假设frame N正在显示，那么为了在下一个VSYNC信号中显示frame N+1，此时frame N+1将被SurfaceFlinger提前获取到。当VSYNC信号到达时，HWC会flip buffer。当app需要绘制frame N+2到buffer时，SurfaceFlinger会遍历每个layer寻找是否有新的buffer，此时没有找到任何新的buffer，因此再下一个VSYNC中再次准备显示frame N+1，过了一点时间之后，app结束了rendering frame N+2并且进入了SurfaceFlinger的队列，可是此时已经太晚了，这种双重buffer的方式效率并不够高。</p>

<ul>
<li><strong>Triple-buffering</strong></li>
</ul>


<p>在VSYNC之前，frame N正在显示，frame N+1已经composited并且准备被显示，frame N+2已经在队列中并且准备好了被SurfaceFlinger获取。当屏幕flip时，buffers可以进行rotate，并且不产生bubble。</p>

<h2>Surface and SurfaceHolder</h2>

<p>Surface通常作为buffer queue的生产者，同时SurfaceFlinger作为消费者。
用来显示Surface的BufferQueue通常是triple-buffering的，但是buffers却是按需分配的。</p>

<h3>Canvas Rendering</h3>

<p>曾经一段时间，所有的rendering渲染操作都是在软件层进行的。在今天，你也是可以这样做。在低版本的系统中，在软件层的渲染是通过使用Skia的图像库来实现的。如果你想要绘制一个矩形，调用一个库函数，在buffer中设置恰当的bytes数据。为了确保buffer不会在同一个时刻被两个Clients所更新，或者在显示时被写入，你必须对buffer进行加锁操作。<code>lockCanvas()</code>方法可以锁住buffer并且返回一个Canvas用来进行绘制。<code>unlockCanvasAndPost()</code>方法可以解锁buffer并且发送buffer数据给compositor进行组合。</p>

<p>随着时间的推移，带有3D引擎的设备出现了，Android围绕OpenGL ES做了修改与适应。然而，为了兼容旧的API，增加了硬件加速Canvas API。</p>

<p>当你为了访问Canvas而锁住Surface时，CPU渲染器会于BufferQueue的生产者建立连接，指导Surface被销毁时才回断开。大多数其他的生产者(例如GLES)可以与Surface进行断开连接与重新连接，但是Canvas-based的CPU渲染器不可以。这意味着如果你不针对一个Canvas进行加锁的话，是不可以在Surface上使用GLES进行绘制，也不可以给Surface发送来自视频解码器中的数据帧。</p>

<p>生产者从BufferQueue中第一次请求一个buffer时，这个buffer是初始化分配为0的。初始化是非常有必要的，避免在不同进程中出现共享数据的意外错误。当你重用一个buffer时，之前的内容还是处于显示中。如果你重复的调用<code>lockCanvas()</code>与<code>unlockCanvasAndPost()</code>却没有绘制任何内容，你将会在前一个frame与渲染的frame中进行循环。</p>

<p>Surface的lock/unlock代码保持了前一个渲染buffer的reference。如果你在锁定Surface的时候指定了一块dirty的区域，它会从之前的buffer中copy一份non-dirty的pixel数据过来。此时buffer将会机会对等的被SurfaceFlinger或者HWC进行处理，但是因为我们仅仅是需要进行读取的操作，所以没有必要进行互斥的访问。</p>

<p>不通过Canvas(non-Canvas)的方式对一个Surface直接进行绘制的主要方式是通过OpenGL ES.这部分的内容会在下面被讲解。</p>

<h3>SurfaceHolder</h3>

<p>连接Surface与SurfaceView的是SurfaceHolder。最开始的想法是Surface代表了原始的数据buffer，那么SurfaceHolder是app用来追踪Surface的一些上层的信息，例如dimensions与format。Java语言定义好了潜在可能的native的实现接口。虽然有争议部分API是不会再可能被使用到的，可是却一直遗留在Public API中没有移除。</p>

<p>通常来说，对于一个View的任何操作都回触发SurfaceHolder。一些其他的APIs，例如MediaCodec，可以直接操作Surface本身。你也可以从SurfaceHolder中获取到Surface。</p>

<p>获取与设置Surface参数的APIs，例如size和format，都是通过SurfaceHolder来实现的。</p>

<h2>EGLSurface and OpenGL ES</h2>

<p>OpenGL ES定义了一个API用来渲染图形。为了是得GLES可以在各种平台上工作，它设计成可以和一个库进行结合的方式来工作。这个库知道如何通过操作系统创建与访问窗口。在Android中使用的库叫做EGL。如果你想要绘制textured polygons(多边形，模块)，你可以使用GLES的方法。如果你想要把渲染绘制到屏幕上，你可以使用EGL的方法。</p>

<p>在开始使用GLES之前，你需要创建一个GL context。在EGL中，这意味着创建一个EGLContext与一个EGLSurface。GLES的操作是作用在当前的context上的，当前的context保存在当前thread中，这个context是不能传递的。这意味着你必需注意渲染的动作执行在哪个线程，并且在那个线程中的context是哪个。</p>

<p>EGLSurface可以通过EGL(执行pbuffer)来分配一个buffer,或者通过操作系统来做一个窗口的分配。EGL window surface是通过<code>eglCreateWindowSurface()</code>方法来创建的。这个方法会使用一个window object作为参数，这个window object在android上可以是一个SurfaceView，一个SurfaceTexture，一个surfaceHolder或者是一个Surface，这些对象的内部都拥有一个BufferQuueue。当你执行了那个方法调用之后，EGL创建一个新的EGLSurface对象，并把这个对象与生产者的BufferQuueue的接口建立连接。</p>

<p>EGL并没有提供lock/unlock的方法。你需要列出绘制的命令然后执行<code>eglSwapBuffer()</code>来提交当前frame。这个方法名本意是描述传统的front-back buffer的swap操作，但是实际上再后续的实现中又可能会有差异。</p>

<p>在同一时刻，只能有一个EGLSurface与Surface进行结合，你只能有一个生产者连接到BufferQueue，但是如果你销毁了EGLSurface，那么就与BufferQueue连接断开，此时可以允许其他组件进行连接。</p>

<p>一个EGLSurface在同一时刻必须只能存在于一个Thread中。但是一个Thread可以切换多个不同的EGLSurface。</p>

<p>讨论到EGLSurface时最通常的误解是认为这只是Surface的某个方面(例如SurfaceHolder).它们实际上是有关联却相互独立的。你可以在ELGSurface上绘制没有被Surface hold住得部分，你也可以使用Surface的时候不要用EGL。EGLSurface只为GLES提供了一个绘制的地方。</p>

<h2>SurfaceView and GLSurfaceView</h2>

<p>从现在起，我们可以探讨下一些更上层的组件，以及它们是如何与底层的组件进行适配的。</p>

<p>Android app Framework UI是基于hierarchy中的View进行搭建的。大多数的文章都没有涉及到细节的讨论，但是了解UI组件是如何经过一系列复杂的measurement与layout然后适配到一个矩形区域是非常有意义的。当app来到forground的时候，所有可见的view对象都将被SurfaceFlinger渲染到由WindowManager创建的Surface中。Layout与rendering是执行在app的UI Thread的。</p>

<p>无论你又多少的layouts与Views，所有的对象都是被绘制到同一个Buffer中。无论Views是否开启了hardware-accelerated都是一样的。</p>

<p>SurfaceView的参数和其他的View一样，你可以设置position与size。但是讨论到render的时候，SurfaceView的contents是透明的。SurfaceView在View中只是一个透明的占位区域。</p>

<p>当SurfaceView可见时，WindowManager会请求SurfaceFlinger创建一个新的Surface。(这个过程不是同步的，所以需要实现Surface被创建的回调用来获取到信息)。默认情况下，新创建的Surface是在app UI Surface的下层(后面)，但是这个默认的Z-ordering可以被override，设置为在app UI的上面(Top表面)。</p>

<p>渲染到Surface上的任何数据都是由SurfaceFlinger进行组合的，而不是由app完成。这就是SurfaceView真正牛B的地方：绘制到SurfaceView上的内容可以由单独的Thread或者Proces进行操作。你可以忽略UI Thread，但是你还是需要保持SurfaceView与Activity的生命周期一致，整个SurfaceView是和app UI还有其他被硬件加速的layer是共同协作的。</p>

<p>请注意，相对于BufferQueue来说，Surface是生产者，而SurfaceFlinger layer是消费者。你可以使用任何合适的方法来更新Surface，只要使得Surface可以作为BufferQueue的生产者。你可以使用Surface提供的Canvas的功能，添加一个EGLSurface并使用GLES进行绘制并且使用MeidaCodec的Video decoder对Surface进行写操作。</p>

<h2>GLSurfaceView</h2>

<p>GLSurfaceView提供了一些帮助类用来协助管理EGL contexts，线程间的交互，与Activity生命周期的互动。</p>

<p>GLSurfaceView创建一个线程用来渲染与配置EGL context。当Activity暂停时，状态会被自动清除。</p>

<p>在大多数情况下，GLSurfaceView是非常有用的并且可以和GLES进行协作。在某些情形下，使用它是个好的选择。</p>

<h2>SurfaceTexture</h2>

<p>SurfaceTexture是在Android 3.0才被引入的。和SurfaceView的概念类似(Surface与View的结合体)，SurfaceTexture是Surface与GLES texture的结合体。</p>

<p>当你创建一个SurfaceTexture时，你会创建一个BufferQueue，app此时扮演了消费者的角色。当新的buffer被生产者加入队列时，你的app会通过<code>onFrameAvailable()</code>的回调得到通知。<code>updateTextureImage()</code>方法会释放前面hold住得buffer，从队列中请求一个新的buffer，并执行一些EGL的方法使得buffer对于GLES来说可以作为一个external texture。</p>

<p>External texture(<code>GL_TEXTURE_EXTERNAL_OES</code>)与GLES(<code>GL_TEXTURE_2D</code>)创建的texture并不相同。你必须对你的渲染器做一些不同的配置，同时有些东西是不能修改的。但是最主要的是：你可以把从BufferQueue中获取到的数据直接渲染为textured模型。</p>

<p>你可能好奇我们如何能够确保在buffer中的数据格式是能够被GLES可以识别的。当SurfaceTexture创建了BufferQueue，它会设置消费者的usge flag为<code>GRALLOC_USAGE_HW_TEXTURE</code>，确保被gralloc创建的任何buffer可以被GLES所使用。</p>

<p>因为SurfaceTexture是需要和EGL context进行交互的，你需要注意从正确的线程中调用SurfaceTexture的方法。</p>

<p>每一个传递的Buffer都不仅仅是buffer本身，还包含了一个timestamp与transformation的信息。</p>

<p>提供transformation的信息是为了提供效率。在某些情况下，对于消费者来说，source data可能是错误的orientation，我们没有在发送数据之前就做方向矫正，而是把数据与它的角度信息一起进行传递。transformation matrix的信息可以在数据被使用的时候与其他transformation信息进行整合，这样能够最大化的减少额外的开销。</p>

<p>timestamp也是很有帮助的。例如，假设你连接到了一个生产者的接口上(通过<code>setPreviewTexture()</code>连接到Camera的输出接口)，如果你想要创建一个Video，你需要为每一帧数据设置当前的timestamp，这个timestamp应该是基于frame被captured来计算的，而不是buffer被接收到得时间。这个timestamp是被camera的代码进行设置的，这样确保了timestamp更加具有连续性。</p>

<h2>SurfaceTexture and Surface</h2>

<p>如果你仔细查看API文档，你将会发现：对于程序来说，创建一个空白Surface的唯一方式是通过它的构造函数来实现，这个构造函数使用SufaceTexture作为唯一的参数。（在API 11之前，还没有public的surface构造器）。如果你把SurfaceTexture作为Surface与Texture的结合体，只有唯一的构造方法就会成为一个缺点。</p>

<p>其实SurfaceTexture可以称为GLConsumer，这能够更加准确的描述它扮演的角色：作为BufferQueue的owner与comsumer。当你从SurfaceTexture中创建一个Surface时，从SurfaceTexture的BufferQueue的角度来看，你是在创建一个对象用来作为生产者。</p>

<h2>TextureView</h2>

<p>TextureView是Android 4.0才开始被引入的。它是很复杂的一个View对象，它会组合View与SurfaceTexture。</p>

<p>SurfaceTexture被称为一个GL comsumer，它会消费图形数据的buffer并使得他们作为texture的形式存在。TextureView对SurfaceTexture做了包装，取代了响应回调的职责并负责请求新的buffer。新的buffer会导致TextureView进行invalidate的操作。当请求绘制时，TextureView使用最近接收到得buffer作为它的source data，渲染时不用考虑View的任何状态值。</p>

<p>你可以使用GLES对TextureView进行渲染，就像SurfaceView一样。仅仅是把SurfaceTexture传递到EGL的window创建的方法里面。然而这样做，暴露了一个隐藏的问题。</p>

<p>在前面我们有提到，BufferQueues可以在不同的进程中传递buffers。当使用GLES渲染TextureView的时候，producer与consumer是在同一个进程，并且他们可能会执行在同一个thread。假设我们从UI Thread快速的提交了一连串的buffers。EGL buffer swap的方法会需要从BufferQueue中执行dequeue的操作，他会一直停留直到有一个buffer可用。可是直到consumer请求一个buffer进行渲染之前，不会有任何的buffer可用，而且这件事情也发生在UI Thread。所以这就出现问题了。</p>

<p>解决方案是使得BufferQueue确保总是有一个可用的buffer用来进行dequeue，这样避免buffer swap会被卡住。其中一个方法是当新的buffer进行queued操作时，BufferQueue能够discard前一个queued的buffer，并且确保queue中存在的最少buffer数同时限制加入队列中得最多的buffer个数。(<strong>如果你的队列中，有三个buffers，消费者一次请求了所有的三个buffer，那么就没有buffer用来出队了，buffer swap的调用会被卡住或者失败，隐藏我们需要阻止消费者一次请求超过2个buffer。</strong>)错失Buffer通常是不希望发生的事情，因此只有在特殊情况下才允许发生，例如生产者与消费者在同一个Process。</p>

<h2>SurfaceView or TextureView</h2>

<p>SurfaceView与TextureView扮演了相似的角色，但在实现原理上却有着非常大的差异。选择哪个才是最好的需要知道如何进行权衡利弊。</p>

<p>TextureView是View hierarchy中一个普通的成员，它的行为和其他View类似，可以叠加到其他View上也可以被其他View进行重叠覆盖。你可以对它执行任意的transformation，也可以通过简单的API调用获取其中的数据内容。</p>

<p>TextureView的主要缺陷是composition步骤的效率。对于SurfaceView来说，数据内容是由SurfaceFlinger绘制到单独的一个layer上的，完美的实现了层叠。对于TextureView，View的composition是由GLES执行的，同时更新TextureView的数据内容可能会导致其他View组件也出现redraw(如果view是放在TextureView的上面的话)。在View渲染成功之后，app UI layer必须通过SurfaceFlinger与其他layer(StatusBar layer,NavigationBar layer)进行composition。对于一个全屏的视频播放应用，还有那些UI元素全部在video layer上面的应用，SurfaceView提供了更好的性能。</p>

<p>前面有提到过，对于RDM保护的Video只能在上层的layer上呈现。能够播放DRM的Video Player必须使用SurfaceView。</p>

<h2>SurfaceView and Activity Lifecycle</h2>

<p>当使用SurfaceView的时候，渲染Surface应该使用单独的Thread而不是Main UI Thread。这会带来一些thread与activity生命周期的交互问题。</p>

<p>首先，对于一个拥有SurfaceView的Activity，他们有两个互相依赖的生命周期：</p>

<ul>
<li>App onCreate/onResume/onPause</li>
<li>Surface created/changed/destoryed</li>
</ul>


<p>当Activity启动时，你会按下面的顺序接收到callback:</p>

<ul>
<li>onCreate</li>
<li>onResume</li>
<li>surfaceCreated</li>
<li>surfaceChanged</li>
</ul>


<p>如果你点击back按钮，你将会收到下面的callback:</p>

<ul>
<li>onPause</li>
<li>surfaceDestoryed(在Surface消失之前就会执行)</li>
</ul>


<p>如果你点击电源按钮来关闭屏幕，你只会收到<code>onPause()</code>，没有<code>surfaceDestroyed()</code>。此时Surface仍然是存活的，渲染操作还可以继续进行。如果你有需要，还可以持续请求获取到Choreographyer的信号。如果你在锁屏的情况下旋转屏幕，然后对设备进行解锁，此时Activity会被重新启动。但是如果没有进行旋转，解锁之后还是能够获取到之前的Surface。</p>

<p>当在SurfaceView里面使用一个单独的渲染线程时有一个问题：这个Thread的生命周期是不是应该和Surface或者Activity进行绑定？答案取决于在屏幕关闭时你想要做的操作。有两个方法：(1)在Activity的start/stop时对Thread做start/stop的操作。(2)在Surface的create/destory时对thread做start/stop的操作。</p>

<ul>
<li>方法(1)是和app的生命周期进行绑定。在onResume()的时候开启渲染线程，在onPause时停止它。这会遇到一点麻烦是：不知道何时创建于配置thread，因为Surface有时存在有时又不在。我们必须等待Surface被创建时候才能做一些线程中的初始化的操作。但是我们还不能简单的把这些操作放到<code>surfaceCreated()</code>的回调里面执行，因为如果surface不被重新创建的话，就没有办法触发那些初始化的操作。因此我们不要查询与保存Surface的状态，并且把这些信息传递给渲染thread。请注意，在多核的系统中，线程间传递数据需要小心谨慎，最好的方式是通过Handler message来传递Surface或者SurfaceHolder，而不仅仅是把他们放到thread里面。</li>
<li>方法(2)呼声更高，因为Surface与渲染的逻辑是相关联的。在Surface创建之后开启渲染Thread，这样避免了一些线程间通信的问题。Surface created/changed的消息都是先于Thread收到的。我们需要确保屏幕关闭之后，渲染操作会停止，在屏幕点亮之后操作能够恢复。可以通过简单的通知Choreographer来停止触发frame绘制的回调。仅仅只有在渲染thread正在运行，我们的<code>onReusme</code>方法才需要恢复那些callback。如果我们基于frame之间的时间做动画的操作，中间省略掉得一些数据是不要紧的。给出明确的pause/resume的消息是个不错的写法。</li>
</ul>


<p>=====
学习自：<a href="http://source.android.com/devices/graphics/architecture.html">http://source.android.com/devices/graphics/architecture.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Android Notes(06) - Camera]]></title>
    <link href="http://hukai.me/blog/android-notes-06-camera/"/>
    <updated>2014-04-21T21:24:00+08:00</updated>
    <id>http://hukai.me/blog/android-notes-06-camera</id>
    <content type="html"><![CDATA[<p>Android framework为各种不同的Camera与Camera的特色功能提供了支持，使得可以在应用中进行拍照与录像。这篇文章会讨论一种简便，快速的拍照录像方式，为了给用户创建定制的相机体验，文章也会概述相机的高级功能。</p>

<h2>0)开始之前</h2>

<p>在应用中开启Android设备的相机功能之前，应该考量如下几个问题：</p>

<ul>
<li><strong>必须的相机硬件</strong> - 当然不能把一个包含相机功能的应用安装到一个连相机硬件都没有的设备上。因此，应该在mainfest文件中声明需要使用到相机。</li>
<li><strong>快速获取图片还是定制相机</strong> - 应用将如何使用相机？是想做一个快速的抓拍还是录制一小段视频剪辑？还是说想提供一种新的相机使用方式？如果是快速的获取一张抓拍图片或者是一小段视频剪辑，建议查看下面的<strong>3)使用已经安装的相机应用。</strong>如果是为了开发一个定制相机功能的应用，查看下面的<strong>4)创建一个相机应用</strong>。</li>
<li><strong>存储位置</strong> - 生成的图片与视频是只对自己的应用可见还是其它类似Gallery的应用也可以访问？即使自己的应用被卸载后也不能被其他应用访问吗？建议查看<strong>5)保存媒体文件</strong></li>
</ul>


<h2>1)简要概述</h2>

<p>Android framework通过提供Camera API来支持拍照与录制视频的功能。下面是相关的类：</p>

<ul>
<li><a href="http://developer.android.com/reference/android/hardware/Camera.html"><strong>Camera</strong></a><br/>
该类是控制相机硬件的基础的API。它可以用来拍照或者录制视频。</li>
<li><a href="http://developer.android.com/reference/android/view/SurfaceView.html"><strong>SurfaceView</strong></a><br/>
该类是用来呈现一个动态的相机预览界面。</li>
<li><a href="http://developer.android.com/reference/android/media/MediaRecorder.html"><strong>MediaRecorder</strong></a><br/>
该类用来使用相机录制视频。</li>
<li><a href="http://developer.android.com/reference/android/content/Intent.html"><strong>Intent</strong></a><br/>
使用<a href="http://developer.android.com/reference/android/provider/MediaStore.html#ACTION_IMAGE_CAPTURE">MediaStore.ACTION_IMAGE_CAPTURE</a> 或者 <a href="http://developer.android.com/reference/android/provider/MediaStore.html#ACTION_VIDEO_CAPTURE">MediaStore.ACTION_VIDEO_CAPTURE</a>作为Intent的action可以用来拍照与录制视频。</li>
</ul>


<!-- More -->


<h2>2)Manifest声明</h2>

<p>在使用Camera API开发应用之前，应该确保应用的mainfest中有做恰当的声明，表明需要使用相机或者是相机的相关功能。</p>

<ul>
<li><p><strong>Camera Permission</strong> - 为了使用相机硬件，你的应用必须请求使用Camera的权限。<br/>
<code>xml
&lt;uses-permission android:name="android.permission.CAMERA" /&gt;
</code><br/>
<strong>Note:</strong>如果你是通过Intent来使用Camera，你的应用程序则不需要请求这个权限。</p></li>
<li><p><strong>Camera Features</strong> - 你的应用还必须声明使用相机功能，例如：<br/>
<code>xml
&lt;uses-feature android:name="android.hardware.camera" /&gt;
</code><br/>
关于相机功能列表，请参考<a href="http://developer.android.com/guide/topics/manifest/uses-feature-element.html#hw-features">功能引用</a>。增加相机功能到你的mainfest文件，这样Google Play可以阻止那些没有相机硬件或者没有相机特定功能的设备安装你的应用。关于Google Play如何做过滤的信息，请参考<a href="http://developer.android.com/guide/topics/manifest/uses-feature-element.html#market-feature-filtering">Google Play and Feature-Based Filtering</a>。<br/>
你还可以为每个相机特性设置<code>android:required</code>的属性，表示这个功能是否为必须的。</p></li>
<li><p><strong>Storage Permission</strong> - 如果你的应用需要保存图片或者视频到设备的外置存储空间(SD card)上，你也需要在manifest中指定存取权限。<br/>
<code>xml
&lt;uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" /&gt;
</code></p></li>
<li><p><strong>Audio Recording Permission</strong> - 为了录制音频或者视频，你的程序必须请求audio capture permission。<br/>
<code>xml
&lt;uses-permission android:name="android.permission.RECORD_AUDIO" /&gt;
</code></p></li>
<li><strong>Location Permission</strong> - 如果你的应用需要为图片添加位置信息，你还需要请求location permission:<br/>
<code>xml
&lt;uses-permission android:name="android.permission.ACCESS_FINE_LOCATION" /&gt;
</code><br/>
关于获取用户位置信息的更多细节，请参考<a href="http://developer.android.com/guide/topics/location/strategies.html">Location Strategies</a>.</li>
</ul>


<h2>3)Using Existing Camera Apps</h2>

<p>在你的应用中快速的实现拍照与录制视频的方法是使用一个Intent来调用已经存在系统中的相机程序。通过已经存在的相机程序拍照或者录制视频，然后返回数据给请求方。这一部分会演示如何使用这种技术。</p>

<p>触发Camera Intent需要遵守如下几个步骤：</p>

<ul>
<li><p><strong>Compose a Camera Intent</strong> - 创建一个请求拍照或者录像的Intent，使用下面的intent类型：</p>

<ul>
<li><a href="http://developer.android.com/reference/android/provider/MediaStore.html#ACTION_IMAGE_CAPTURE">MediaStore.ACTION_IMAGE_CAPTURE</a> - 请求拍照的Intent。</li>
<li><a href="http://developer.android.com/reference/android/provider/MediaStore.html#ACTION_VIDEO_CAPTURE">MediaStore.ACTION_VIDEO_CAPTURE</a> - 请求录像的Intent。</li>
</ul>
</li>
<li><p><strong>Start the Camera Intent</strong> - 使用<a href="http://developer.android.com/reference/android/app/Activity.html#startActivityForResult(android.content.Intent,%20int">startActivityForResult()</a>)方法来执行这个Intent。在启动这个Intent之后，相机程序会被唤起并提供拍照或者录像的功能。</p></li>
<li><p><strong>Receive the Intent Result</strong> - 在你的程序里面实现<a href="http://developer.android.com/reference/android/app/Activity.html#onActivityResult(int,%20int,%20android.content.Intent">onActivityResult()</a>)的方法用来接收相机程序返回的数据。当用户结束拍照或者录像之后，系统会调用到这个方法。</p></li>
</ul>


<h3>3.1)Image capture intent</h3>

<p>使用Camera Intent是一种使用最少的代码为你的程序开启拍照功能的一种简便的方法。一个拍照程序可以包含下面的附加信息：</p>

<p><strong><a href="http://developer.android.com/reference/android/provider/MediaStore.html#EXTRA_OUTPUT">MediaStore.EXTRA_OUTPUT</a></strong> - 这定义了一个Uri对象来指定存放图片的路径与文件名。这个设置信息是可选的，但是强烈建议添加。如果你不指定这个值，相机程序会使用默认的文件名保存图片到默认的位置，这个值可以从Intent.getData()的字段中获取到。</p>

<p>下面的示例代码演示了如何构建一个拍照Intent并执行它。<code>getOutputMediaFileUri()</code>方法可以从<strong>Saving Media Files</strong>的段落中涉及到。</p>

<p>```java
private static final int CAPTURE_IMAGE_ACTIVITY_REQUEST_CODE = 100;
private Uri fileUri;</p>

<p>@Override
public void onCreate(Bundle savedInstanceState) {</p>

<pre><code>super.onCreate(savedInstanceState);
setContentView(R.layout.main);

// create Intent to take a picture and return control to the calling application
Intent intent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);

fileUri = getOutputMediaFileUri(MEDIA_TYPE_IMAGE); // create a file to save the image
intent.putExtra(MediaStore.EXTRA_OUTPUT, fileUri); // set the image file name

// start the image capture Intent
startActivityForResult(intent, CAPTURE_IMAGE_ACTIVITY_REQUEST_CODE);
</code></pre>

<p>}
```</p>

<p>当startActivityForResult()方法被执行，用户会看到一个相机拍照的界面。用户执行了拍照(或者取消操作)，用户界面会回退到你的程序，你必须在onActivityResult()方法里面接收返回的数据。关于如何接受完整的intent，可以参考下面的<strong>Receiving camera intent result</strong>段落。</p>

<h3>3.2)Video capture intent</h3>

<p>视频录制的原理和拍照一致。一个视频录制的Intent可以包含如下的参数信息：</p>

<ul>
<li><a href="http://developer.android.com/reference/android/provider/MediaStore.html#EXTRA_OUTPUT">MediaStore.EXTRA_OUTPUT</a> - 和拍照类似，这里指定保存视频的位置。同样这个字段是可选的，但是也被强烈建议进行填写。如果没有传递这个参数，相机程序会使用默认的文件名保存文件到默认的存储位置。你可以通过在返回的Intent.getData()字段中获取到这个值。</li>
<li><a href="http://developer.android.com/reference/android/provider/MediaStore.html#EXTRA_VIDEO_QUALITY">MediaStore.EXTRA_VIDEO_QUALITY</a> - 这里的值可以为0或者1，分别表示低质量与高质量。</li>
<li><a href="http://developer.android.com/reference/android/provider/MediaStore.html#EXTRA_DURATION_LIMIT">MediaStore.EXTRA_DURATION_LIMIT</a> - 设置这个值用来限制视频的长度，用毫秒计算。</li>
<li><a href="http://developer.android.com/reference/android/provider/MediaStore.html#EXTRA_SIZE_LIMIT">MediaStore.EXTRA_SIZE_LIMIT</a> - 设置这个值用来限制文件的大小，用btye做单位。</li>
</ul>


<p>下面演示了如何构建一个Video Intent并执行：
```java
private static final int CAPTURE_VIDEO_ACTIVITY_REQUEST_CODE = 200;
private Uri fileUri;</p>

<p>@Override
public void onCreate(Bundle savedInstanceState) {</p>

<pre><code>super.onCreate(savedInstanceState);
setContentView(R.layout.main);

//create new Intent
Intent intent = new Intent(MediaStore.ACTION_VIDEO_CAPTURE);

fileUri = getOutputMediaFileUri(MEDIA_TYPE_VIDEO);  // create a file to save the video
intent.putExtra(MediaStore.EXTRA_OUTPUT, fileUri);  // set the image file name

intent.putExtra(MediaStore.EXTRA_VIDEO_QUALITY, 1); // set the video image quality to high

// start the Video Capture Intent
startActivityForResult(intent, CAPTURE_VIDEO_ACTIVITY_REQUEST_CODE);
</code></pre>

<p>}
```
和拍照类似，也需要在activity的onActivityResult里面去接收数据并做处理。</p>

<h3>3.3)Receiving camera intent result</h3>

<p>一旦你构建并执行了一个拍照或者录像的Intent，你的程序必须确保能够正确接收返回的数据。为了正确的接收到Intent，你必须重写onActivityResult()的方法，下面会演示如何获取到上面示例代码返回的数据。</p>

<p>```java
private static final int CAPTURE_IMAGE_ACTIVITY_REQUEST_CODE = 100;
private static final int CAPTURE_VIDEO_ACTIVITY_REQUEST_CODE = 200;</p>

<p>@Override
protected void onActivityResult(int requestCode, int resultCode, Intent data) {</p>

<pre><code>if (requestCode == CAPTURE_IMAGE_ACTIVITY_REQUEST_CODE) {
    if (resultCode == RESULT_OK) {
        // Image captured and saved to fileUri specified in the Intent
        Toast.makeText(this, "Image saved to:\n" +
                 data.getData(), Toast.LENGTH_LONG).show();
    } else if (resultCode == RESULT_CANCELED) {
        // User cancelled the image capture
    } else {
        // Image capture failed, advise user
    }
}

if (requestCode == CAPTURE_VIDEO_ACTIVITY_REQUEST_CODE) {
    if (resultCode == RESULT_OK) {
        // Video captured and saved to fileUri specified in the Intent
        Toast.makeText(this, "Video saved to:\n" +
                 data.getData(), Toast.LENGTH_LONG).show();
    } else if (resultCode == RESULT_CANCELED) {
        // User cancelled the video capture
    } else {
        // Video capture failed, advise user
    }
}
</code></pre>

<p>}
```</p>

<p>一旦你的activity成功接收了数据，那么你的程序就可以在指定的位置获取到图片或者视频了。</p>

<h2>4)Building a Camera App</h2>

<p>一些开发者也许需要开发一个定制的相机应用，用来提供特殊的功能与体验。创建一个定制的相机界面比起使用Intent需要更多的代码，但是它能够提供一种更加优秀的用户体验。</p>

<p>通常来说创建一个定制化的相机界面有如下几个步骤：</p>

<ul>
<li><strong>Detect and Access Camera</strong> - 检查相机是否存在并可访问。</li>
<li><strong>Create a Preview Class</strong> - 创建一个继承自SurfaceView的preview类，并implement SurfaceHolder的接口的interface。这个类用来预览相机的动态图片。</li>
<li><strong>Build a Preview Layout</strong> - 一旦你拥有了preview class。创建一个Layout用来承载preview并提供交互控制界面。</li>
<li><strong>Setup Listeners for Capture</strong> - 为控制界面建立监听器，用来启动拍照或者录像。</li>
<li><strong>Capture and Save Files</strong> - 建立拍照录像的代码并进行保存。</li>
<li><strong>Release the Camera</strong> - 使用完相机之后，你的程序必须正确的释放它，以便其他程序使用。</li>
</ul>


<p>相机硬件是一个共享资源，它必须被小心谨慎的管理使用。因此你的程序不应该和其他可能使用相机硬件的程序有冲突。下面的段落会介绍如何检测相机硬件，如何请求获取权限，如何拍照录像以及如何在使用完毕时释放相机。</p>

<p><strong>注意:</strong> 当你的程序执行完任务之后，需要记得通过执行Camera.release()来释放相机对象。如果你的相机没有合理的释放相机，后续包括你自己的应用在内的所有的相机应用，都将无法正常打开相机并且可能导致程序崩溃。</p>

<h3>4.1)Detecting camera hardware</h3>

<p>如果你的程序没有在manifest中声明需要使用相机，你应该在运行时去检查相机是否可用。为了执行这个检查，需要使用到<a href="http://developer.android.com/reference/android/content/pm/PackageManager.html#hasSystemFeature(java.lang.String">PackageManager.hasSystemFeature()</a>) 方法，如下所示：</p>

<p>```java
/<em>* Check if this device has a camera </em>/
private boolean checkCameraHardware(Context context) {</p>

<pre><code>if (context.getPackageManager().hasSystemFeature(PackageManager.FEATURE_CAMERA)){
    // this device has a camera
    return true;
} else {
    // no camera on this device
    return false;
}
</code></pre>

<p>}
```</p>

<p>Android设备可以拥有多个摄像头，例如前置与后置摄像头。从Android 2.3 (API Level 9)开始，可以通过<a href="http://developer.android.com/reference/android/hardware/Camera.html#getNumberOfCameras(">Camera.getNumberOfCameras()</a>)方法获取到摄像头的个数。</p>

<h3>4.2)Accessing cameras</h3>

<p>如果你已经判断到程序运行的设备上有摄像头，你需要获取到摄像头的话，需要通过一个相机实例来进行访问。</p>

<p>为了访问到主摄像头，如下所示，使用<a href="http://developer.android.com/reference/android/hardware/Camera.html#open(">Camera.open()</a>)方法。
```java
/<em>* A safe way to get an instance of the Camera object. </em>/
public static Camera getCameraInstance(){</p>

<pre><code>Camera c = null;
try {
    c = Camera.open(); // attempt to get a Camera instance
}
catch (Exception e){
    // Camera is not available (in use or does not exist)
}
return c; // returns null if camera is unavailable
</code></pre>

<p>}
```</p>

<p><strong>注意:</strong> 当使用Camera.open方法时总是需要做检查exceptions的动作。如果没有检查exception，有可能会因为相机正在使用或者相机不存在而使得程序崩溃。</p>

<p>在Android 2.3 (API Level 9)开始, 你可以使用通过<a href="http://developer.android.com/reference/android/hardware/Camera.html#open(int">Camera.open(int)</a>)方法来访问特定的摄像头。上面演示的代码会优先获取主摄像头。</p>

<h3>4.3)Checking camera features</h3>

<p>一旦你获取到相机，你可以使用<a href="http://developer.android.com/reference/android/hardware/Camera.html#getParameters(">Camera.getParameters()</a>)方法来获取到更多的相机信息。也可以通过获取到的相机参数对象得到相机能够支持的功能。从android 2.3开始，使用<a href="http://developer.android.com/reference/android/hardware/Camera.html#getCameraInfo(int,%20android.hardware.Camera.CameraInfo">Camera.getCameraInfo()</a>)可以获取到相机是前置还是后置摄像头以及拍摄出来的图片角度。</p>

<h3>4.4)Creating a preview class</h3>

<p>为了给用户提供有效的拍照与录像体验，用户需要能够看到摄像头捕获的数据。相机预览是使用SurfaceView，它能够显示来自摄像头的数据，因此用户可以分割捕获图片或者视频。</p>

<p>下面的示例代码演示了如何创建一个基础的相机预览类，该类可以included到另外一个layout中。为了捕获拍照事件的回调，需要implement<a href="http://developer.android.com/reference/android/view/SurfaceHolder.Callback.html">SurfaceHolder.Callback</a> ，之后可以在这些回调里面进行创建与销毁View的操作。</p>

<p>```java
/<em>* A basic Camera preview class </em>/
public class CameraPreview extends SurfaceView implements SurfaceHolder.Callback {</p>

<pre><code>private SurfaceHolder mHolder;
private Camera mCamera;

public CameraPreview(Context context, Camera camera) {
    super(context);
    mCamera = camera;

    // Install a SurfaceHolder.Callback so we get notified when the
    // underlying surface is created and destroyed.
    mHolder = getHolder();
    mHolder.addCallback(this);
    // deprecated setting, but required on Android versions prior to 3.0
    mHolder.setType(SurfaceHolder.SURFACE_TYPE_PUSH_BUFFERS);
}

public void surfaceCreated(SurfaceHolder holder) {
    // The Surface has been created, now tell the camera where to draw the preview.
    try {
        mCamera.setPreviewDisplay(holder);
        mCamera.startPreview();
    } catch (IOException e) {
        Log.d(TAG, "Error setting camera preview: " + e.getMessage());
    }
}

public void surfaceDestroyed(SurfaceHolder holder) {
    // empty. Take care of releasing the Camera preview in your activity.
}

public void surfaceChanged(SurfaceHolder holder, int format, int w, int h) {
    // If your preview can change or rotate, take care of those events here.
    // Make sure to stop the preview before resizing or reformatting it.

    if (mHolder.getSurface() == null){
      // preview surface does not exist
      return;
    }

    // stop preview before making changes
    try {
        mCamera.stopPreview();
    } catch (Exception e){
      // ignore: tried to stop a non-existent preview
    }

    // set preview size and make any resize, rotate or
    // reformatting changes here

    // start preview with new settings
    try {
        mCamera.setPreviewDisplay(mHolder);
        mCamera.startPreview();

    } catch (Exception e){
        Log.d(TAG, "Error starting camera preview: " + e.getMessage());
    }
}
</code></pre>

<p>}
```</p>

<p>如果你想为你的相机预览界面设置特定的预览大小，可以在<code>surfaceChanged()</code>的回调里面进行操作(注意上面演示代码的注释)。设置预览大小时，你<strong>必须</strong>使用从<a href="http://developer.android.com/reference/android/hardware/Camera.Parameters.html#getSupportedPreviewSizes(">getSupportedPreviewSizes()</a>)方法获取到的预览值，不能在<a href="http://developer.android.com/reference/android/hardware/Camera.Parameters.html#setPreviewSize(int,%20int">setPreviewSize()</a>)方法里设置随意的预览值。</p>

<h3>4.5)Placing preview in a layout</h3>

<p>在上一段落演示的Camera Preview Class,必须放置在一个activity的layout中。这一段落会演示为了预览如何创建一个基础的layout与activity。</p>

<p>下面的代码提供了一个能够显示相机预览界面的基础layout。在这段代码中，FrameLayout是相机预览类的container。使用framelayout可以在相机预览界面上叠加额外的图片信息或者是操作控制组件。</p>

<p>```xml
&lt;?xml version="1.0" encoding="utf-8"?>
&lt;LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"</p>

<pre><code>android:orientation="horizontal"
android:layout_width="fill_parent"
android:layout_height="fill_parent"
&gt;
</code></pre>

<p>  &lt;FrameLayout</p>

<pre><code>android:id="@+id/camera_preview"
android:layout_width="fill_parent"
android:layout_height="fill_parent"
android:layout_weight="1"
/&gt;
</code></pre>

<p>  &lt;Button</p>

<pre><code>android:id="@+id/button_capture"
android:text="Capture"
android:layout_width="wrap_content"
android:layout_height="wrap_content"
android:layout_gravity="center"
/&gt;
</code></pre>

<p></LinearLayout>
```</p>

<p>在大多数设备上，相机预览的角度默认是横屏的。演示的layout指定了horizontal，并且下面的代码使得activity固定成横屏的模式。</p>

<p>```xml
&lt;activity android:name=".CameraActivity"</p>

<pre><code>      android:label="@string/app_name"

      android:screenOrientation="landscape"&gt;
      &lt;!-- configure this activity to use landscape orientation --&gt;

      &lt;intent-filter&gt;
    &lt;action android:name="android.intent.action.MAIN" /&gt;
    &lt;category android:name="android.intent.category.LAUNCHER" /&gt;
&lt;/intent-filter&gt;
</code></pre>

<p></activity>
```</p>

<p><strong>Note:</strong> 相机预览界面不一定是要横屏的。从android 2.2开始，你可以使用<a href="http://developer.android.com/reference/android/hardware/Camera.html#setDisplayOrientation(int">setDisplayOrientation()</a>)方法来设置预览图片的角度。为了在用户旋转手机时改变相机预览的角度，在<code>surfaceChanged()</code>方法里面，首先要使用<a href="http://developer.android.com/reference/android/hardware/Camera.html#stopPreview(">Camera.stopPreview()</a>)停止预览，然后再使用<a href="http://developer.android.com/reference/android/hardware/Camera.html#startPreview(">Camera.startPreview()</a>)方法来重新开启相机预览。</p>

<p>为了在activity中添加相机界面，你的Camera Activity必须确保在activity pause或者是destory的时候释放相机资源。下面的代码演示了如何添加camera preview class到camera activity中。</p>

<p>```java
public class CameraActivity extends Activity {</p>

<pre><code>private Camera mCamera;
private CameraPreview mPreview;

@Override
public void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.main);

    // Create an instance of Camera
    mCamera = getCameraInstance();

    // Create our Preview view and set it as the content of our activity.
    mPreview = new CameraPreview(this, mCamera);
    FrameLayout preview = (FrameLayout) findViewById(R.id.camera_preview);
    preview.addView(mPreview);
}
</code></pre>

<p>}
```</p>

<p><strong>Note:</strong> 上面演示的<code>getCameraInstance()</code>方法出现在4.2)Accessing camera段落中。</p>

<h3>4.6)Capturing pictures</h3>

<p>一旦你建立了preview class并且创建好了显示的layout。那么就可以开始做拍照的动作了。</p>

<p>为了获取到一张图片，需要使用<a href="http://developer.android.com/reference/android/hardware/Camera.html#takePicture(android.hardware.Camera.ShutterCallback,%20android.hardware.Camera.PictureCallback,%20android.hardware.Camera.PictureCallback">Camera.takePicture()</a>)方法。为了获取到JPEG格式的图片数据，你必须implement一个<a href="http://developer.android.com/reference/android/hardware/Camera.PictureCallback.html">Camera.PictureCallback</a>接口来接收图片数据并把它写到文件中。</p>

<p>```java
private PictureCallback mPicture = new PictureCallback() {</p>

<pre><code>@Override
public void onPictureTaken(byte[] data, Camera camera) {

    File pictureFile = getOutputMediaFile(MEDIA_TYPE_IMAGE);
    if (pictureFile == null){
        Log.d(TAG, "Error creating media file, check storage permissions: " +
            e.getMessage());
        return;
    }

    try {
        FileOutputStream fos = new FileOutputStream(pictureFile);
        fos.write(data);
        fos.close();
    } catch (FileNotFoundException e) {
        Log.d(TAG, "File not found: " + e.getMessage());
    } catch (IOException e) {
        Log.d(TAG, "Error accessing file: " + e.getMessage());
    }
}
</code></pre>

<p>};
```</p>

<p>触发拍照的动作，需要使用下面演示到的方法。</p>

<p>```java
// Add a listener to the Capture button
Button captureButton = (Button) findViewById(id.button_capture);
captureButton.setOnClickListener(</p>

<pre><code>new View.OnClickListener() {
    @Override
    public void onClick(View v) {
        // get an image from the camera
        mCamera.takePicture(null, null, mPicture);
    }
}
</code></pre>

<p>);
```</p>

<h3>4.7)Capturing videos</h3>

<p>录制视频的内容，暂时跳过，下次再看。</p>

<h3>4.8)Releasing the camera</h3>

<p>相机是被共享的一种资源。获取到一个相机实例之后，程序才可以使用相机，但是，在使用完相机的时候，程序必须小心谨慎的释放它。在程序进入到pause状态时，立即释放相机资源。如果你的程序没有合理的释放相机资源，包括自己程序本身在内，后续所有的相机请求都将失败，甚至可能会导致程序崩溃。下面的代码演示了如何释放相机资源。</p>

<p>```java
public class CameraActivity extends Activity {</p>

<pre><code>private Camera mCamera;
private SurfaceView mPreview;
private MediaRecorder mMediaRecorder;

...

@Override
protected void onPause() {
    super.onPause();
    releaseMediaRecorder();       // if you are using MediaRecorder, release it first
    releaseCamera();              // release the camera immediately on pause event
}

private void releaseMediaRecorder(){
    if (mMediaRecorder != null) {
        mMediaRecorder.reset();   // clear recorder configuration
        mMediaRecorder.release(); // release the recorder object
        mMediaRecorder = null;
        mCamera.lock();           // lock camera for later use
    }
}

private void releaseCamera(){
    if (mCamera != null){
        mCamera.release();        // release the camera for other applications
        mCamera = null;
    }
}
</code></pre>

<p>}
```</p>

<h2>5)Saving Media Files</h2>

<p>用户创建的图片或者视频均需要保存到设备的external storage目录下(SD Card)。可以有多种可能的位置用来保存文件，但是作为一个开发人员，只有下面两种标准的路径进行保存。</p>

<ul>
<li><a href="http://developer.android.com/reference/android/os/Environment.html#getExternalStoragePublicDirectory(java.lang.String">Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_PICTURES)</a>) - 这个方法会返回用来保存图片与视频所推荐使用的标准共享目录。这个目录是共享开放的，所以其他程序可以轻易的发现，读取并删除这个目录下的文件。如果你的程序被用户卸载，在这个目录下的文件不会被移除。为了避免干扰到用户已经存在的图片与视频目录，你应该为你的程序创建一个子目录。如下面的代码所示。这个方法从Android 2.2 (API Level 8)开始就可以使用。</li>
<li><a href="http://developer.android.com/reference/android/content/Context.html#getExternalFilesDir(java.lang.String">Context.getExternalFilesDir(Environment.DIRECTORY_PICTURES)</a>) - 这个方法会返回一个和你的程序相关联的，用来保存图片与视频的，标准目录。如果你的程序被卸载，在这个目录下的文件也会被一起移除。这个目录并不能阻止其他程序的读写。</li>
</ul>


<p>```java
public static final int MEDIA_TYPE_IMAGE = 1;
public static final int MEDIA_TYPE_VIDEO = 2;</p>

<p>/<em>* Create a file Uri for saving an image or video </em>/
private static Uri getOutputMediaFileUri(int type){</p>

<pre><code>  return Uri.fromFile(getOutputMediaFile(type));
</code></pre>

<p>}</p>

<p>/<em>* Create a File for saving an image or video </em>/
private static File getOutputMediaFile(int type){</p>

<pre><code>// To be safe, you should check that the SDCard is mounted
// using Environment.getExternalStorageState() before doing this.

File mediaStorageDir = new File(Environment.getExternalStoragePublicDirectory(
          Environment.DIRECTORY_PICTURES), "MyCameraApp");
// This location works best if you want the created images to be shared
// between applications and persist after your app has been uninstalled.

// Create the storage directory if it does not exist
if (! mediaStorageDir.exists()){
    if (! mediaStorageDir.mkdirs()){
        Log.d("MyCameraApp", "failed to create directory");
        return null;
    }
}

// Create a media file name
String timeStamp = new SimpleDateFormat("yyyyMMdd_HHmmss").format(new Date());
File mediaFile;
if (type == MEDIA_TYPE_IMAGE){
    mediaFile = new File(mediaStorageDir.getPath() + File.separator +
    "IMG_"+ timeStamp + ".jpg");
} else if(type == MEDIA_TYPE_VIDEO) {
    mediaFile = new File(mediaStorageDir.getPath() + File.separator +
    "VID_"+ timeStamp + ".mp4");
} else {
    return null;
}
return mediaFile;
</code></pre>

<p>}
```</p>

<h2>6)Camera Features</h2>

<p>Android提供了控制相机特性的方法，例如图片格式化，闪光灯模式，设置聚焦等等。这一段落列出了通常的相机功能并简短的介绍如何使用这些功能。大多数相机特性可以通过<strong>Camera.Parameters</strong>对象来获取并进行相关的设置。然而，有几个重要的功能不仅仅是通过<strong>Camera.Parameters</strong>能够实现的。请看下面的内容介绍：</p>

<ul>
<li>Metering and focus areas：测光并进行聚焦</li>
<li>Face detection：人脸检测</li>
<li>Time lapse video：延时视频</li>
</ul>


<p>关于上面3个常用的功能会在下面进行更加详细的介绍，除此之外的其他相机功能，请参考下面这张表：</p>

<p><img src="/images/articles/camera_features_table.png" title="Camera Common Features" alt="camera_features_table.png" /></p>

<p><strong>Note:</strong> 因为软硬件的差异性，那些功能并不一定都是支持的。对于检查功能是否可用，请参考下面的Checking feature availability.</p>

<h3>6.1)Checking feature availability</h3>

<p>相机的有些功能在所有手机上并不一定是都支持的。在开发相机应用时就需要提前考虑应该适配到哪个Level。然后开发的时候需要动态的去根据功能是否支持来做不同的处理。</p>

<p>你可以通过获取到相机参数的对象来做检测。下面的例子演示了如何检查autofocus功能是否可用：
```java
// get Camera parameters
Camera.Parameters params = mCamera.getParameters();</p>

<p>List<String> focusModes = params.getSupportedFocusModes();
if (focusModes.contains(Camera.Parameters.FOCUS_MODE_AUTO)) {
  // Autofocus mode is supported
}
```</p>

<p>对于大多数的相机特性，都可以使用类型上面的代码来处理。Camera.Parameters对象提供了一系列的类似<code>getSupported...()</code>, <code>is...Supported()</code> 与 <code>getMax...()</code> 方法来判断某个功能是否可用的。</p>

<p>如果你的程序确定需要相机的某个特性，你可以在mainfest文件中就进声明。例如你声明了flash与auto-focus的功能，那么Google Play会阻止那些不支持这些功能的设备安装这个应用。关于相机功能的声明列表，请参考<a href="http://developer.android.com/guide/topics/manifest/uses-feature-element.html#hw-features">Features Reference.</a></p>

<h3>6.2)Using camera features</h3>

<p>前面已经提到过，通过Camera.Parameters对象来操控相机。如下所示：
<code>java
// get Camera parameters
Camera.Parameters params = mCamera.getParameters();
// set the focus mode
params.setFocusMode(Camera.Parameters.FOCUS_MODE_AUTO);
// set Camera parameters
mCamera.setParameters(params);
</code>
上面这种方法对大多数相机功能都是可用的，在你获取到相机的实例之后，大多数参数都是在任意时间均可以修改的。参数修改的效果在相机预览的界面是可以立即看到效果的。在软件层面，实际上可能是需要花几帧的时间来产生效果的，因为需要发送指令给相机硬件产生效果。</p>

<p><strong>Important:</strong> 部分功能是不能修改的。特别是，修改相机预览的角度与大小，是需要先停止预览的。从Android Android 4.0 (API Level 14)开始，预览角度可以不用重启预览就可以进行修改。</p>

<p>下面快速的介绍前面提到的3个功能：</p>

<h4>6.2.1）Metering and focus areas</h4>

<p>在某些拍照场景下，自动聚焦与测光并不能拍出我们想要的效果。从Android 4.0 (API Level 14)开始，支持指定特定的区域用来focus与metering。下面的代码演示了如何指定特定的区域用来focus与metering。</p>

<p>```java
// Create an instance of Camera
mCamera = getCameraInstance();</p>

<p>// set Camera parameters
Camera.Parameters params = mCamera.getParameters();</p>

<p>if (params.getMaxNumMeteringAreas() > 0){ // check that metering areas are supported</p>

<pre><code>List&lt;Camera.Area&gt; meteringAreas = new ArrayList&lt;Camera.Area&gt;();

Rect areaRect1 = new Rect(-100, -100, 100, 100);    // specify an area in center of image
meteringAreas.add(new Camera.Area(areaRect1, 600)); // set weight to 60%
Rect areaRect2 = new Rect(800, -1000, 1000, -800);  // specify an area in upper right of image
meteringAreas.add(new Camera.Area(areaRect2, 400)); // set weight to 40%
params.setMeteringAreas(meteringAreas);
</code></pre>

<p>}</p>

<p>mCamera.setParameters(params);
```
<a href="http://developer.android.com/reference/android/hardware/Camera.Area.html">Camera.Area</a>对象包含了2个参数: 一个Rect对象用来指定区域，另外一个是权重用来告诉相机这部分区域的重要性。</p>

<p>Rect的值是基于2000*2000的坐标体系。(-1000, -1000)表示的左上角，(1000, 1000)表示的是右下角。如下图所示：</p>

<p><img src="/images/articles/camera-area-coordinates.png" title="Figure 1. The red lines illustrate the coordinate system for specifying a Camera.Area within a camera preview. The blue box shows the location and shape of an camera area with the Rect values 333,333,667,667." alt="camera-area-coordinates.png" /></p>

<p>这个坐标系统是于相机的可见预览区域相吻合的，不会因为预览效果的放大缩小二改变。同样的，通过Camera.setDisplayOrientation()旋转预览并不会导致坐标系统的变化。</p>

<h4>6.2.2)Face detection</h4>

<p>对于包含人像的图片，头像是最重要的区域。在拍照的时候，这个区域应该用来作为focus与while balance操作的重要参数。从Android 4.0 (API Level 14)开始，系统能够提供API来动态检测人脸并且可以使用人脸识别技术来计算拍照图片的设置参数。</p>

<p><strong>Note:</strong> 当人脸检测功能开启的时候，<code>setWhiteBalance(String)</code>, <code>setFocusAreas(List)</code> 与<code>setMeteringAreas(List)</code>并没有效果。</p>

<p>使用人脸检测功能需要下面的几个步骤：</p>

<ul>
<li>检测在设备上是否有人脸检测的功能。</li>
<li>创建一个人脸检测的监听器。</li>
<li>把这个人脸检测器添加到Camera对象中。</li>
<li>在预览开启之后启动人脸检测(每次预览重启也是)。</li>
</ul>


<p>通过<a href="http://developer.android.com/reference/android/hardware/Camera.Parameters.html#getMaxNumDetectedFaces(">getMaxNumDetectedFaces()</a>)方法来获取设备是否支持检测人脸。如下面的<code>startFaceDetection()</code>方法中演示的：</p>

<p>为了把人脸检测到得结果反馈到界面上，还需要实现检测的监听器，如下所示：
```java
class MyFaceDetectionListener implements Camera.FaceDetectionListener {</p>

<pre><code>@Override
public void onFaceDetection(Face[] faces, Camera camera) {
    if (faces.length &gt; 0){
        Log.d("FaceDetection", "face detected: "+ faces.length +
                " Face 1 Location X: " + faces[0].rect.centerX() +
                "Y: " + faces[0].rect.centerY() );
    }
}
</code></pre>

<p>}
```</p>

<p>创建完这个类之后，需要把这个Listener加到Camera的对象中，如下所示：
<code>java
mCamera.setFaceDetectionListener(new MyFaceDetectionListener());
</code></p>

<p>你得程序必须在每次启动或者重启相机预览的时候开始人脸检测。如下所示：
```java
public void startFaceDetection(){</p>

<pre><code>// Try starting Face Detection
Camera.Parameters params = mCamera.getParameters();

// start face detection only *after* preview has started
if (params.getMaxNumDetectedFaces() &gt; 0){
    // camera supports face detection, so can start it:
    mCamera.startFaceDetection();
}
</code></pre>

<p>}
```</p>

<p>如果使用前面4.4)Creating a preview class段落中提到的预览类，你需要在<code>surfaceCreated()</code> 与 <code>surfaceChanged()</code>方法中执行<code>startFaceDetection()</code>方法。如下所示：
```java
public void surfaceCreated(SurfaceHolder holder) {</p>

<pre><code>try {
    mCamera.setPreviewDisplay(holder);
    mCamera.startPreview();

    startFaceDetection(); // start face detection feature

} catch (IOException e) {
    Log.d(TAG, "Error setting camera preview: " + e.getMessage());
}
</code></pre>

<p>}</p>

<p>public void surfaceChanged(SurfaceHolder holder, int format, int w, int h) {</p>

<pre><code>if (mHolder.getSurface() == null){
    // preview surface does not exist
    Log.d(TAG, "mHolder.getSurface() == null");
    return;
}

try {
    mCamera.stopPreview();

} catch (Exception e){
    // ignore: tried to stop a non-existent preview
    Log.d(TAG, "Error stopping camera preview: " + e.getMessage());
}

try {
    mCamera.setPreviewDisplay(mHolder);
    mCamera.startPreview();

    startFaceDetection(); // re-start face detection feature

} catch (Exception e){
    // ignore: tried to stop a non-existent preview
    Log.d(TAG, "Error starting camera preview: " + e.getMessage());
}
</code></pre>

<p>}
```
<strong>Note:</strong> 要在startPreview()方法之后才能开启人脸检测。请不要尝试在onCreate()方法里面启动人脸检测。</p>

<h4>6.2.3)Time lapse video</h4>

<p>Time lapse video使得用户可以通过组合一段时间的图片生成视频片段。这个功能利用了<a href="http://developer.android.com/reference/android/media/MediaRecorder.html">MediaRecorder</a>每隔一段时间来记录一张图片。</p>

<p>为了实现这个功能，你需要像配置一个普通的recorder对象一样。如下面的代码所示：
<code>java
// Step 3: Set a CamcorderProfile (requires API Level 8 or higher)
mMediaRecorder.setProfile(CamcorderProfile.get(CamcorderProfile.QUALITY_TIME_LAPSE_HIGH));
...
// Step 5.5: Set the video capture rate to a low number
mMediaRecorder.setCaptureRate(0.1); // capture a frame every 10 seconds
</code>
上面的设置只是对MediaRecorder进行参数设置的一小部分。一旦配置结束，就可以开启视频录制。关于更多的信息，请参考前面的4.7)Capture Video(还没有开始学习)</p>

<p>===</p>

<p>学习自<a href="http://developer.android.com/guide/topics/media/camera.html">http://developer.android.com/guide/topics/media/camera.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Android Training(18) - 使用IntentService执行任务(Lesson 3 - 回传任务状态给发送方)]]></title>
    <link href="http://hukai.me/blog/android-training-18-running-background-service-lesson-3/"/>
    <updated>2014-04-20T22:01:00+08:00</updated>
    <id>http://hukai.me/blog/android-training-18-running-background-service-lesson-3</id>
    <content type="html"><![CDATA[<p>这章节会演示如何回传IntentService中执行的任务状态与结果给发送方。 例如，回传任务的状态给Activity并进行更新UI。推荐的方式是使用<a href="http://developer.android.com/reference/android/support/v4/content/LocalBroadcastManager.html">LocalBroadcastManager</a>，这个组件可以限制broadcast只在自己的App中进行传递。</p>

<h2>Report Status From an IntentService</h2>

<p>为了在IntentService中向其他组件发送任务状态，首先创建一个Intent并在data字段中包含需要传递的信息。作为一个可选项，还可以给这个Intent添加一个action与data URI。</p>

<p>下一步，通过执行<a href="http://developer.android.com/reference/android/support/v4/content/LocalBroadcastManager.html#sendBroadcast(android.content.Intent">LocalBroadcastManager.sendBroadcast()</a>))来发送Intent。Intent被发送到任何有注册接受它的组件中。为了获取到LocalBroadcastManager的实例，可以执行getInstance().代码示例如下：</p>

<p>```java
public final class Constants {</p>

<pre><code>...
// Defines a custom Intent action
public static final String BROADCAST_ACTION =
    "com.example.android.threadsample.BROADCAST";
...
// Defines the key for the status "extra" in an Intent
public static final String EXTENDED_DATA_STATUS =
    "com.example.android.threadsample.STATUS";
...
</code></pre>

<p>}
public class RSSPullService extends IntentService {
...</p>

<pre><code>/*
 * Creates a new Intent containing a Uri object
 * BROADCAST_ACTION is a custom Intent action
 */
Intent localIntent =
        new Intent(Constants.BROADCAST_ACTION)
        // Puts the status into the Intent
        .putExtra(Constants.EXTENDED_DATA_STATUS, status);
// Broadcasts the Intent to receivers in this app.
LocalBroadcastManager.getInstance(this).sendBroadcast(localIntent);
</code></pre>

<p>...
}
```</p>

<!-- More -->


<p>下一步是在发送任务的组件中接收发送出来的broadcast数据。</p>

<h2>Receive Status Broadcasts from an IntentService</h2>

<p>为了接受广播的数据对象，需要使用BroadcastReceiver的子类并实现<a href="http://developer.android.com/reference/android/content/BroadcastReceiver.html#onReceive(android.content.Context,%20android.content.Intent">BroadcastReceiver.onReceive()</a>)的方法，这里可以接收LocalBroadcastManager发出的广播数据。</p>

<p>```java
// Broadcast receiver for receiving status updates from the IntentService
private class ResponseReceiver extends BroadcastReceiver
{</p>

<pre><code>// Prevents instantiation
private DownloadStateReceiver() {
}
// Called when the BroadcastReceiver gets an Intent it's registered to receive
@
public void onReceive(Context context, Intent intent) {
</code></pre>

<p>...</p>

<pre><code>    /*
     * Handle Intents here.
     */
</code></pre>

<p>...</p>

<pre><code>}
</code></pre>

<p>}
```</p>

<p>一旦定义了BroadcastReceiver，也应该定义actions，categories与data用来做广播过滤。为了实现这些，需要使用<a href="http://developer.android.com/reference/android/content/IntentFilter.html">IntentFilter</a>.如下所示：</p>

<p>```java
// Class that displays photos
public class DisplayActivity extends FragmentActivity {</p>

<pre><code>...
public void onCreate(Bundle stateBundle) {
    ...
    super.onCreate(stateBundle);
    ...
    // The filter's action is BROADCAST_ACTION
    IntentFilter mStatusIntentFilter = new IntentFilter(
            Constants.BROADCAST_ACTION);

    // Adds a data filter for the HTTP scheme
    mStatusIntentFilter.addDataScheme("http");
</code></pre>

<p>```</p>

<p>为了给系统注册这个BroadcastReceiver，需要通过LocalBroadcastManager执行registerReceiver()的方法。如下所示：</p>

<p>```java
// Instantiates a new DownloadStateReceiver</p>

<pre><code>    DownloadStateReceiver mDownloadStateReceiver =
            new DownloadStateReceiver();
    // Registers the DownloadStateReceiver and its intent filters
    LocalBroadcastManager.getInstance(this).registerReceiver(
            mDownloadStateReceiver,
            mStatusIntentFilter);
    ...
</code></pre>

<p>```</p>

<p>一个BroadcastReceiver可以处理多种类型的广播数据。每个广播数据都有自己的ACTION。这个功能使得不用定义多个不同的BroadcastReceiver来分别处理不同的ACTION数据。为BroadcastReceiver定义另外一个IntentFilter，只需要创建一个新的IntentFilter并重复执行registerReceiver()即可。例如:</p>

<p>```java
/*</p>

<pre><code>     * Instantiates a new action filter.
     * No data filter is needed.
     */
    statusIntentFilter = new IntentFilter(Constants.ACTION_ZOOM_IMAGE);
    ...
    // Registers the receiver with the new filter
    LocalBroadcastManager.getInstance(getActivity()).registerReceiver(
            mDownloadStateReceiver,
            mIntentFilter);
</code></pre>

<p>```</p>

<p>发送一个广播并不会start或者resume一个Activity。BroadcastReceiver可以接收广播数据，即使是你的app是在后台运行中。但是这不会强迫使得你的app变成foreground的。如果想在app不可见的时候通知用户一个后台的事件，建议使用<a href="http://developer.android.com/reference/android/app/Notification.html">Notification</a>。永远不要为了响应一个广播而去启动Activity。</p>

<hr />

<p><strong>后记：</strong>使用LocalBroadcastManager结合IntentService其实是一种很典型高效的做法，同时也更符合OO的思想，通过广播注册与反注册的方式，对两个组件进行解耦。如果使用Handler传递到后台线程作为回调，容易带来的内存泄漏。原因是：匿名内部类对外面的Actvitiy持有引用，如果在Acitivity被销毁的时候，没有对Handler进行显式的解绑，会导致Activity无法正常销毁，这样自然就有了内存泄漏。当然，如果用文章中的方案，通常也要记得在Activity的onPause的时候进行unRegisterReceiver，除非你有充足的理由为解释这里为何要继续保留。</p>

<hr />

<p><strong>学习自<a href="http://developer.android.com/training/run-background-service/report-status.html">http://developer.android.com/training/run-background-service/report-status.html</a>,欢迎交流讨论</strong></p>
]]></content>
  </entry>
  
</feed>
